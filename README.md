# Event-Driven User Activity Service

This project is a foundational microservice that works as both:

- a Kafka producer (publishes user activity events)
- a Kafka consumer (reads and processes those same events)

It is designed to demonstrate practical event-driven architecture patterns used in distributed systems:

- asynchronous communication through Kafka
- clear producer/consumer separation inside one service
- idempotent event processing
- basic resiliency (producer retries + consumer-safe error handling)
- containerized local environment with Docker Compose

---

## 1) What this service does

### Producer flow

1. Client calls `POST /events/generate`
2. Service validates input (`userId`, `eventType`, optional `payload`)
3. Service generates:
   - `eventId` (UUID)
   - `timestamp` (ISO 8601)
4. Service publishes full event to Kafka topic: `user-activity-events`

### Consumer flow

1. Consumer subscribes to topic `user-activity-events`
2. Consumer group: `user-activity-consumer-group`
3. Every message is parsed + schema-checked
4. If valid and not already processed, event is stored in memory
5. For each consumed event, service logs `eventId`, `userId`, `eventType`

### Query flow

- `GET /events/processed` returns all unique processed events currently stored in memory

---

## 2) Event schema

Published/consumed event format:

```json
{
  "eventId": "UUID_STRING",
  "userId": "STRING",
  "eventType": "LOGIN | LOGOUT | PRODUCT_VIEW",
  "timestamp": "ISO_8601_STRING",
  "payload": {}
}
```

Rules:

- `eventId` is generated by service
- `timestamp` is generated by service
- `eventType` must be one of `LOGIN`, `LOGOUT`, `PRODUCT_VIEW`
- `payload` must be a JSON object when provided

---

## 3) Tech stack

- Node.js 20
- Express
- KafkaJS
- Jest + Supertest
- Docker + Docker Compose
- Kafka + Zookeeper (Confluent images)

---

## 4) Project structure

```text
.
├── src
│   ├── app.js
│   ├── bootstrap.js
│   ├── config.js
│   ├── index.js
│   ├── events
│   │   └── userEvent.js
│   ├── kafka
│   │   ├── consumer.js
│   │   ├── kafkaClient.js
│   │   └── producer.js
│   ├── services
│   │   └── eventProcessor.js
│   └── store
│       └── processedEventsStore.js
├── tests
│   ├── integration
│   │   └── eventFlow.integration.test.js
│   └── unit
│       ├── eventProcessor.test.js
│       ├── eventPublisher.test.js
│       └── kafkaProducer.test.js
├── .env.example
├── Dockerfile
├── docker-compose.yml
├── jest.config.js
└── README.md
```

---

## 5) Configuration

Copy `.env.example` if you want local non-docker overrides.

| Variable                          | Purpose                       | Default                              |
| --------------------------------- | ----------------------------- | ------------------------------------ |
| `NODE_ENV`                        | Runtime environment           | `development`                        |
| `PORT`                            | HTTP port                     | `3000`                               |
| `KAFKA_CLIENT_ID`                 | Kafka client identifier       | `event-driven-user-activity-service` |
| `KAFKA_BROKERS`                   | Comma-separated Kafka brokers | `localhost:9092`                     |
| `KAFKA_TOPIC`                     | Topic for user events         | `user-activity-events`               |
| `KAFKA_CONSUMER_GROUP`            | Consumer group ID             | `user-activity-consumer-group`       |
| `KAFKA_PRODUCER_RETRIES`          | Producer retry attempts       | `3`                                  |
| `KAFKA_PRODUCER_RETRY_BACKOFF_MS` | Retry backoff base            | `500`                                |
| `KAFKA_CONNECTION_TIMEOUT_MS`     | Kafka connection timeout      | `10000`                              |
| `KAFKA_REQUEST_TIMEOUT_MS`        | Kafka request timeout         | `30000`                              |

---

## 6) Run with Docker Compose

### Start all services

```bash
docker compose up --build
```

Services started:

- `zookeeper`
- `kafka`
- `app-service`

Health checks are configured for all three services.

### Stop services

```bash
docker compose down
```

To remove volumes/network artifacts too:

```bash
docker compose down -v
```

---

## 7) API reference

Base URL (local): `http://localhost:3000`

### Health

**GET** `/health`

Response:

```json
{ "status": "ok" }
```

### Generate event

**POST** `/events/generate`

Request body:

```json
{
  "userId": "user-1001",
  "eventType": "PRODUCT_VIEW",
  "payload": {
    "productId": "sku-7788",
    "category": "electronics"
  }
}
```

Success response (`201 Created`):

```json
{
  "eventId": "7f8d9d4a-9058-4de8-8824-36431448c76f"
}
```

Validation error (`400 Bad Request`):

```json
{
  "message": "Invalid request payload",
  "errors": [
    "userId is required and must be a non-empty string",
    "eventType must be one of: LOGIN, LOGOUT, PRODUCT_VIEW"
  ]
}
```

Internal publish failure (`500 Internal Server Error`):

```json
{
  "message": "Failed to publish event"
}
```

### Get processed events

**GET** `/events/processed`

Success response (`200 OK`):

```json
[
  {
    "eventId": "7f8d9d4a-9058-4de8-8824-36431448c76f",
    "userId": "user-1001",
    "eventType": "PRODUCT_VIEW",
    "timestamp": "2026-02-17T10:00:00.000Z",
    "payload": {
      "productId": "sku-7788",
      "category": "electronics"
    }
  }
]
```

---

## 8) Quick verification steps

1. Start stack:

   ```bash
   docker compose up --build
   ```

2. Generate a few events:

   ```bash
   curl -X POST http://localhost:3000/events/generate \
   	 -H "Content-Type: application/json" \
   	 -d '{"userId":"user-1","eventType":"LOGIN","payload":{"ip":"10.0.0.10"}}'
   ```

   ```bash
   curl -X POST http://localhost:3000/events/generate \
   	 -H "Content-Type: application/json" \
   	 -d '{"userId":"user-1","eventType":"PRODUCT_VIEW","payload":{"productId":"p-10"}}'
   ```

3. Query processed events:

   ```bash
   curl http://localhost:3000/events/processed
   ```

4. Watch logs to see consumer output (`eventId`, `userId`, `eventType`):

   ```bash
   docker compose logs -f app-service
   ```

---

## 9) Testing

All tests are in `tests/`.

### Unit tests

Covers:

- request/event validation logic
- producer publish retry behavior
- idempotent event processing/store behavior

Run inside container:

```bash
docker compose exec app-service npm run test:unit
```

### Integration tests

Covers:

- `POST /events/generate` publishes to Kafka
- consumer receives events and stores them
- `GET /events/processed` returns consumed events
- duplicate `eventId` is processed once (idempotency)
- invalid API payload returns `400`

Run inside container:

```bash
docker compose exec app-service npm run test:integration
```

Run everything:

```bash
docker compose exec app-service npm run test:all
```

---

## 10) Design decisions

- Single service with clear internal boundaries (API, producer, consumer, store)
- In-memory store is intentional for scope simplicity and idempotency demonstration
- Producer includes retry with incremental backoff for transient failures
- Consumer is defensive: malformed/invalid messages are logged and skipped safely
- Configuration is environment-driven (no hardcoded deployment-specific values)

---

## 11) Notes for production extension

If this were promoted beyond a learning/assessment project, the next upgrades would normally be:

- persistent storage for processed events
- dead letter topic for bad messages
- metrics/tracing (Prometheus/OpenTelemetry)
- stronger schema strategy (e.g., schema registry + versioning)
- horizontal scaling and partition strategy by key

Current implementation intentionally keeps scope focused on core event-driven requirements.
